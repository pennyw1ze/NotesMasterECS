### Timestamp
Concurrency based on timestamp. We define a total order based on the order in which the schedule are received from the scheduler.
The scheduler timestamps the transactions in increasing order and execute transactions following timestamps order.

For each element X we keep the following data:
- **rts(X)** next X reader;
- **wts(X)** next X writer;
- **wts-c(X)** last X writer;
- **cb(X)** a bit that is false if the transaction who wrote X lastly has not committed yet.

The system manages 2 temporal axes, **physical** and **logical** time.
The phisical time is when an action of a transaction occurs in a schedule, the logical time is the "transaction order" in which they appear in the schedule itself.
To see if transactions are compatible with each other we must have non conflicting order between phisical and logical time i.e. no transaction that has greater phisical time must have lower logical time or viceversa.
When we find a conflict, the conflicting transaction is aborted and executed again later with a new timestamp.

Also with timestamp based method we do not avoid the possibility of deadlock.
- Timestamp-based method is superior when transactions are “read-only”, or when concurrent transactions rarely write the same elements;
- 2PL is superior when the number of conflicts is high because the probability of rollback is higher in the timestamp-based method;

### Transactions in SQL
We have 3 anomalies in SQL:
- Dirty read:
- Non repeatable read: when a transaction read same element twice;
- Phantom read: this happens when we operate on a range of tuples, and in the same time another transaction operates on that tuples. Then our range operation will be disturbed. We need to range lock the elements;

---

### Pages and records
The dimension of a page is usually the dimension of a block.
A **page** has an address and is constituted by a set of slots. A **slot** is a memory space that contains a record of a relation.

A **file** is a collection of pages. Usually all records in a page belongs to the same relation.
Usually a file organizatio supports the following operations:
- **insert**;
- **delete**;
- **update**;
- **read**;
- **scan**;

# File organization

## Simple
### Heap
In a heap file organization the file representing the relation contains a set of pages, each with a set of records. There is **no order criteria** in this structure.
We can imagine the structure of heap files as a linked lists of pages, where there are 2 lists: the first contains the full pages, and the second contains the free pages.

Pages of the same relation are associated into **buckets**.
A set of fields of the relation is choosen as **search key**.
To search for a relation, we can imagine the heap file as an array, and using search key hashed to index the relations stored in the file.

##### Costs
Where B is the number of pages of the file;
- **Insertion**: O(1);
- **Deletion**: O(1);
- **Update**: O(1);
- **Scan**: O(B);
- Equality selection:

### Sorted
**Search key**: set of attributes on which the relation is sorted.
Since the pages in the file are sorted we have clever ways to search for an element.
- **Scan**: O(B): Still have to look into every page;
- **Equality selection**: means searching for an element, so in the avarage case is (Olog(B));
- **Insertion**: O(B) we have to sort all the array;
- **Deletion**: O(B);


### Hashed file
- **Scan:** O(B);
- **Equality selection:** O(1);
- **Range selection:** O(B);
- **Insertion:** O(1);
- **Deletion:** O(1);

### Sorting 
Now we wander if a user wants to receive an ordered result of a query or for whatever other reason, we might need to have ordered data stored.
But how do we order data ?
It is not as simple as it looks because data are stored in secondary storage so accessing data is slow. We need more efficient algorithms. We might do this job with the help of a special element: **the buffer**.
We want to do sorting based on pages, not on records!
We could run merge sort on our buffer that stores pages 2 at a time and merge them.
In this way we will pay $2\times B\times (log_2(B) +1)$.
In this way our buffer is only using 3 frames, 2 to store inputs and 1 to store merged output.

How can we **reduce the number of passes**?
Well that happens if we increase the number of buffers. In this way, we can merge more block in the same step. This will increase the base of the logarithm of B in the cost equation.

So the new cost will be:$$2\times B \times (log_{F-1}B)$$
## Index
An index is a method that given a property is able to find data with that property as value.
The property can be any, not just the key of the relation.
The index have several properites.

#### Organization
We can have:
- Sorted index;
- Tree based index;
- Hash-based index;

### Data entry structure
The data entry could be:
- Data records;
- Pair (k,r) where r is a reference to a data with search key k;
- Pair (k,r-list) where r-list is a list of references to data records with search key k;

### Clustering
An index is **clustering** (strongly clustering) when its data entries are sored with the same order as data records.
We say that an index is **weakly clustering** if all the data indexed for a certain search key appears in the same page.

We can also say  (easy definition):
>An index is **clustered** only if the data records are sorted in the data file according to the order of the values of the search key.

### Primary/Secondary
A  **primary index** is an index on a relation R whose search key includes the primary key of R.
Otherwise it is called **secondary index**.
A primary index cannot contain duplicates.
A secondary index is called unique if its search key contains a non-primary key.

### Sparse/dense
An index is **dense** if every value of the search key appears in at least one data entry of the index.
It is **sparse** if it is not dense.
A sparse index is **clustered**, therefore we will have at most one sparse index per data file.
**Strongly dense** indexes have exactly one data entry for each data record of the data file.
Typically a sparse index have a data entry per page.

### Single/composite
A key is called **single** if it is composed by a single field, otherwise it is called **composite**.
With a single query in a composite index we are able to extract more informations.

## Indexed file organization
### Sorted index
Basically what we've seen before, the idea here is to create an auxiliary sorted file (index) which contains the values for the search key and pointers to records in the data file.
In this case, the binary search can be performed on a smaller file (the indexes file).

![[Pasted image 20250704144036.png]]

- **Clustering** version: the data file is sorted on the same search key of the indexes;
- **Non-clustering** version: the data file is unsorted or sorted on different attribute wrt the indexes;

#### Clustering version
##### Unique version
Unique version means that for each key is associated only one relation.
The clustering sorted index can be sparse or dense. In the dense case we can use **binary search**. But since an index takes less spaces, the index page is smaller.
With a sparse index (that contains one entry per page), we will have that the data entry points to the first record of the page.
So:
- Dense:
  - Can tell if record exists without accessing it;
  - Requires more space;
- Sparse:
  - Less space, can keep more index in memory;
  - Checking record exists require page access;
##### Non-unique version
This means that values of the search key can represent more relations, so we can have duplicates.
In the dense mode we will have more pointers with same values of the key, we could have buckets (lists), or simply different copy of the pointer with the same key pointing to different entries. In either case we will have to examine all the entries.
If the page access are still too much we can have a multilevel sparse index structure, with multiple layer of pointers in order to reduce the dimension of the final index file. The higher level of this structure must be sparse tho, because a dense index would still keep the same number of entries and would not bring any advantage on the dimension of the data structure.

#### Non-clustering version

##### Primary
In a non clustering version with primary search key, a sparse index does not make any sense because the order of the records is not defined well. So we keep only dense index.
This could also apply the multiple level paradigm, and in this case we will adopt a sparse index on the higher level which points not to the data records, but to other index files which indeed have a predetermined order.

##### Secondary
Now there might be duplicates in our non clustering structure. How do we do?
We introduce the concept of buckets. Bucktes here are used to point to all possible records all over the record file which matches to a certain search key. The index file will point to different buckets associated with search key. The index are dense since for each index correspond one bucket (the contrary is not true).

We can also cluster more relations in the same file. Usually we said that files are used to store the same relations, but it could happen that a file is used to store multiple relations, and this is called **CLUSTERING**.

### Tree index
We can have a tree-index based organization. What's the idea here ?
We have a tree structure that stores the sorted search key in order to increase performance on element search operations.
Deletion and insertion are expensive in this case.
In the tree, pages with data entries are the leaves of the tree. Any search start from the root and end to the leaf. Link between nodes are pointer between pages.
We have 2 types of tree structures:
#### ISAM
Indexed sequential access mode:
- The structure is **static**;
- **Data entries** are the leafs;
- Is a **balanced** tree;
- **Fun-out**: nodes have the same number of leaves, which is named fun-out;
Usually the number of funout is 100, such that when at height 4, it has 100 million leaves.
**COSTS**:
- **Index creation**: Linear cost O(1), leaves are inserted sequentially;
- **Search**: $log_F(N)$ where N is the number of leaves and F is the fun-out;
- Insertion and deletion are rare since is a static structure, but requires the allocation of an overflow page or delete the record;

ISAM example:

![[Pasted image 20250704163324.png]]

#### B+ tree
Is again a balanced tree, where the lenght of the path from root to leaf is the same for all nodes.
We define the **rank** of the tree the number of search key values that can be found in a page.
The B+ trees are still balanced, but the number of children for every node may not be the same: every node contain a number of data entries $m_i\ s.t.$ $${(d+1)\over 2}\le m_i \le d$$
where $d$ is the rank of the tree.
We need to keep the occupancy rate at 66% in order to have an efficient tree structure.

How to search trough B+ tree ?
Search a leaf with the first value in the range: